{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ac66dc8-353d-4024-9531-4a1ddd18aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy, torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors, colormaps\n",
    "#brain visualization import\n",
    "from nilearn import regions, datasets, surface, plotting, image, maskers\n",
    "from nilearn.plotting import plot_roi, plot_stat_map\n",
    "#ridge regression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "#from library\n",
    "import sys\n",
    "sys.path.append('/home/maellef/git/cNeuromod_encoding_2020')\n",
    "from models import encoding_models as encod\n",
    "import files_utils as fu\n",
    "from Datasets_utils import SequentialDataset, create_usable_audiofmri_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580cfbfa-faf8-4349-aa2c-971521f084a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIST_path = '/home/maellef/DataBase/fMRI_parcellations/MIST_parcellation/Parcellations/MIST_ROI.nii.gz'\n",
    "voxel_mask = '/home/maellef/git/cNeuromod_encoding_2020/parcellation/STG_middle.nii.gz'\n",
    "dir_path = \"/home/maellef/Results/Phantom_general\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099eea4e-b37c-4057-8d3e-6651c661dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxels_nii(voxel_data, voxel_mask, t_r=1.49):\n",
    "#from voxels to nii\n",
    "    voxel_masker = maskers.NiftiMasker(mask_img=voxel_mask, standardize=False, \n",
    "                                       detrend=False, t_r=t_r, smoothing_fwhm=8)\n",
    "    voxel_masker.fit()\n",
    "    vox_data_nii = voxel_masker.inverse_transform(voxel_data)\n",
    "    return vox_data_nii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe10007-2ca8-4529-aeac-155b26195440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_fig(parcel_data, vmax, threshold=0, cmap='turbo', inflate=True, colorbar=True, no_background=True):     \n",
    "    nii_data = regions.signals_to_img_labels(parcel_data, MIST_path)\n",
    "    fig, ax = plotting.plot_img_on_surf(nii_data,\n",
    "                              views=['lateral', 'medial'], hemispheres=['left', 'right'], inflate=inflate,\n",
    "                              vmax=vmax, threshold=threshold, colorbar=colorbar, cmap=cmap, symmetric_cbar=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920baacb-1e66-406a-a1fa-11e30cec0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_map(voxel_data, vmax=None, cut_coords=None, tr = 1.49, bg_img=None, cmap = 'cold_hot') : \n",
    "    f = plt.Figure()\n",
    "    data_nii = voxels_nii(voxel_data, voxel_mask, t_r=tr)\n",
    "    if bg_img is not None : \n",
    "        plotting.plot_stat_map(data_nii, bg_img=bg_img, draw_cross=False, vmax=vmax,\n",
    "                           display_mode='x', cut_coords=[-63, -57, 57, 63], figure=f,\n",
    "                              black_bg=True, dim = 0, cmap=cmap)\n",
    "    else :\n",
    "        plotting.plot_stat_map(data_nii, draw_cross=False, vmax=vmax,\n",
    "                           display_mode='x', cut_coords=[-63, -57, 57, 63], figure=f)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae84fcc0-7cf5-4f1d-b04a-ec7bfab7a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_colormap(original_colormap = 'twilight', percent_start = 0.25, percent_finish = 0.25):\n",
    "    colormap = colormaps[original_colormap]\n",
    "    nb_colors = colormap.N\n",
    "    new_colors_range = colormap(numpy.linspace(0,1,nb_colors))\n",
    "\n",
    "    n_start = round(nb_colors/(1-percent_start)) - nb_colors if percent_start != 0 else 0\n",
    "    new_color_start = numpy.array([colormap(0)]*n_start).reshape(-1, new_colors_range.shape[1])\n",
    "    n_finish = round(nb_colors/(1-percent_finish)) - nb_colors if percent_finish != 0 else 0\n",
    "    new_color_finish = numpy.array([colormap(0)]*n_finish).reshape(-1, new_colors_range.shape[1])\n",
    "\n",
    "    new_colors_range = numpy.concatenate((new_color_start,new_colors_range,new_color_finish), axis=0)\n",
    "    new_colormap = colors.ListedColormap(new_colors_range)\n",
    "    return new_colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a2924-f5ca-4d67-87de-e7098284d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['friends', 'movie10']:\n",
    "    subdatasets = ['s04'] if dataset == 'friends' else ['bourne', 'figures', 'life', 'wolf']\n",
    "    for sub_ds in subdatasets:\n",
    "        dspath = os.path.join(dir_path, dataset, sub_ds)\n",
    "        for file in os.listdir(dspath):\n",
    "            sub = file[:6]\n",
    "            conv = 'conv4' if 'conv4' in file else 'baseline'\n",
    "            scale = 'WB' if 'MIST_ROI' in file else 'STG'\n",
    "            \n",
    "            filepath = os.path.join(dspath, file)\n",
    "            arr = numpy.load(filepath)\n",
    "            print(arr.shape)\n",
    "\n",
    "            parcel_data = numpy.mean(arr, axis=0).reshape(1, -1)\n",
    "            vmax = numpy.max(numpy.abs(parcel_data))\n",
    "            print(dataset, sub_ds, sub, scale, conv)\n",
    "            print(f'min :', numpy.min(parcel_data), f', max : ', numpy.max(parcel_data), f', absolute max : ', vmax)\n",
    "\n",
    "            #cmp = extend_colormap(original_colormap = 'turbo', percent_start=0.5, percent_finish=0) \n",
    "            if scale == 'WB' :\n",
    "                fig = surface_fig(parcel_data, vmax=None, threshold = 0.1)\n",
    "            else :\n",
    "                fig = voxel_map(parcel_data)\n",
    "        \n",
    "            output_file = os.path.join(dir_path, 'maps', '{}_{}_{}_{}_{}'.format(dataset, sub_ds, sub, scale, conv))\n",
    "            fig.savefig(output_file, dpi=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19c0098a-3d57-46b2-a9f4-718816f3465f",
   "metadata": {},
   "source": [
    "Step 1 load model A and corresponding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33c0045b-4d14-44ca-bf95-66c14215f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = '/home/maellef/Results/best_models/converted' \n",
    "dataset_path = '/home/maellef/DataBase/fMRI_Embeddings' \n",
    "stimuli_path = '/home/maellef/DataBase/stimuli'  \n",
    "outpath = '/home/maellef/Results'\n",
    "save_path = os.path.join(outpath, 'General_phantom')\n",
    "os.makedirs(save_path, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f815e9-a254-4478-8855-e4d0be449c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub_models(sub, scale, conv, models_path, no_init=False): \n",
    "    models = {}\n",
    "    #scale_path = os.path.join(models_path, sub, scale)\n",
    "    for model in os.listdir(models_path):\n",
    "        if '.pt' in model and conv in model and sub in model and scale in model:\n",
    "            model_path = os.path.join(models_path, model)\n",
    "            modeldict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            model_net = encod.SoundNetEncoding_conv(out_size=modeldict['out_size'],output_layer=modeldict['output_layer'],\n",
    "                                                    kernel_size=modeldict['kernel_size'], no_init=no_init)\n",
    "            if not no_init:\n",
    "                model_net.load_state_dict(modeldict['checkpoint'])\n",
    "            models[model] = model_net\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee3db6cc-e860-48b1-8767-a1cc86bf7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoding matrice from last encoding layer : 1024 X 210\n"
     ]
    }
   ],
   "source": [
    "no_init = False\n",
    "conv = 'conv4' #'opt110_wb'\n",
    "sub = 'sub-01'#, 'sub-02', 'sub-03', 'sub-04', 'sub-05'\n",
    "scale = 'MIST_ROI'#, 'auditory_Voxels' \n",
    "\n",
    "models = load_sub_models(sub, scale, conv, models_path, no_init=no_init)\n",
    "\n",
    "dataset = 'friends'\n",
    "sub_dataset = 's04'\n",
    "data_stimuli_path = os.path.join(stimuli_path, dataset, sub_dataset)\n",
    "parcellation_path = os.path.join(dataset_path, scale, dataset, sub)\n",
    "pairs_wav_fmri = fu.associate_stimuli_with_Parcellation(data_stimuli_path, parcellation_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa00b0-f9a8-4f3a-aab3-ce05f4b44124",
   "metadata": {},
   "source": [
    "ste 2.1 get output of model conv 7\n",
    "step 2.2 train ridge regression on Friends s4 with output of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23412821-3f40-4a2a-a97a-a6181d41f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(testloader,net, epoch, mseloss, gpu=True):\n",
    "    all_fmri = []\n",
    "    all_fmri_p = []\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for (wav,fmri) in testloader:\n",
    "            bsize = wav.shape[0]\n",
    "            \n",
    "            # load data\n",
    "            wav = torch.Tensor(wav).view(1,1,-1,1)\n",
    "            if gpu:\n",
    "                wav = wav.cuda()\n",
    "\n",
    "            fmri = fmri.view(bsize,-1)\n",
    "            if gpu:\n",
    "                fmri=fmri.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            fmri_p = net(wav, epoch).permute(2,1,0,3).squeeze()\n",
    "\n",
    "            #Cropping the end of the predicted fmri to match the measured bold\n",
    "            fmri_p = fmri_p[:bsize]\n",
    "            \n",
    "            all_fmri.append(fmri.cpu().numpy().reshape(bsize,-1))\n",
    "            all_fmri_p.append(fmri_p.cpu().numpy().reshape(bsize,-1))\n",
    "\n",
    "    r2_model = r2_score(np.vstack(all_fmri),np.vstack(all_fmri_p),multioutput='raw_values')\n",
    "    return r2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5193a9ab-919d-4bb2-bcb7-0de7ced621c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maellef/git/cNeuromod_encoding_2020/Datasets_utils.py:99: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  length = librosa.get_duration(filename = audio_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n",
      "getting audio files for test...\n",
      "getting fMRI files for test...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "shape = 210 if scale == 'MIST_ROI' else 556\n",
    "for name, model in models.items():\n",
    "    pass\n",
    "batchsize = int(name[name.find('conv_')+len('conv_'):name.find('conv_')+len('conv_')+3])\n",
    "\n",
    "for pair in pairs_wav_fmri:\n",
    "    eval_input = [pair] if len(pair) == 2 else [(pair[0], pair[1])]\n",
    "    xTest, yTest = create_usable_audiofmri_datasets(eval_input, tr=1.49, sr=22050, name='test')\n",
    "    TestDataset = SequentialDataset(xTest, yTest, batch_size=batchsize, selection=None)\n",
    "    testloader = DataLoader(TestDataset, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833c8ce-59d2-427f-a47b-fad30e2bbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_run_eval(sub, dataset, models_dict, pairs_wav_fmri, no_init=False, outpath = save_path):\n",
    "    shape = 210 if scale == 'MIST_ROI' else 556\n",
    "    for name, model in models_dict.items():\n",
    "        all_runs = np.array([]).reshape(-1,shape)\n",
    "        batchsize = int(name[name.find('conv_')+len('conv_'):name.find('conv_')+len('conv_')+3])\n",
    "        print('batchsize '+str(batchsize))\n",
    "        for pair in pairs_wav_fmri:\n",
    "            eval_input = [pair] if len(pair) == 2 else [(pair[0], pair[1])]\n",
    "            xTest, yTest = create_usable_audiofmri_datasets(eval_input, tr=1.49, sr=22050, name='test')\n",
    "            TestDataset = SequentialDataset(xTest, yTest, batch_size=batchsize, selection=None)\n",
    "            testloader = DataLoader(TestDataset, batch_size=None)    \n",
    "            r2_score = test_r2(testloader, net=model, epoch=1, mseloss=nn.MSELoss(reduction='sum'), gpu=False)\n",
    "            all_runs = np.append(all_runs, r2_score.reshape(1,-1), axis=0)\n",
    "        print(all_runs.shape)\n",
    "\n",
    "        #sub_path = os.path.join(save_path, dataset, sub)\n",
    "        #sub_path = sub_path if not no_init else os.path.join(sub_path, 'no_init_model')\n",
    "        #os.makedirs(sub_path, exist_ok=True)\n",
    "        sub_path = outpath\n",
    "        savepath = os.path.join(sub_path, name[:-3])\n",
    "        np.save(savepath, all_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa4d81-9a8e-4057-8615-bfae82a2cd62",
   "metadata": {},
   "source": [
    "step 3 evaluate ridge regrssion result on a film from Movie 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef75f3-e3b2-4ab5-a8a8-f341396ea7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'friends' #, 'movie10'\n",
    "sub_dataset = 's04' #, 'bourne', 'figures', 'life', 'wolf'\n",
    "data_stimuli_path = os.path.join(stimuli_path, dataset, sub_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc77aec-5e30-4111-8adc-40dc68f6a0cc",
   "metadata": {},
   "source": [
    "step 4 visualize predicted output by ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f24089-c8f5-44aa-bcc5-fcfd3a06169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = os.path.join(save_path, dataset, sub_dataset)\n",
    "os.makedirs(result_path, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d3726-1018-48a9-8a0c-2a2b1042ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(0.1, 3, 10)\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    cv = group_kfold.split(X, y, groups)\n",
    "\n",
    "    model = RidgeCV(\n",
    "        alphas=alphas,\n",
    "        fit_intercept=True,\n",
    "        #normalize=False,\n",
    "        cv=cv,\n",
    "    )\n",
    "\n",
    "    return model.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
