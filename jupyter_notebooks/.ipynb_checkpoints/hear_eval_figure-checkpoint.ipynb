{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef610bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3807a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths + global variables\n",
    "subs = ['sub-01','sub-02', 'sub-03','sub-04', 'sub-05', 'sub-06']\n",
    "ft_models = ['conv4', 'conv5', 'conv6', 'conv7', 'no']\n",
    "atlas = ['wholebrain', 'STG']\n",
    "\n",
    "csv_paths_sub236 = '/home/maelle/GitHub_repositories/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/metrics.csv'\n",
    "csv_path_sub145 = '/home/maelle/GitHub_repositories/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/metrics_145.csv'\n",
    "csv_path_leaderboard = '/home/maelle/GitHub_repositories/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/leaderboard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9d4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with difference in accuracy with baseline (no_ft) in reference \n",
    "def diff_df(df, subs, atlas):\n",
    "    all_diff_df = pd.DataFrame([], columns=df.columns)\n",
    "    for sub in subs:\n",
    "        for a in atlas:\n",
    "            selected_df = df.loc[(df['subject'] == sub) & (df['atlas'] == a)]\n",
    "            subject_serie = selected_df.pop('subject').reset_index()\n",
    "            atlas_serie = selected_df.pop('atlas').reset_index()\n",
    "            finetune_serie = selected_df.pop('finetune').reset_index()\n",
    "            diff_df = pd.DataFrame(selected_df.values-selected_df.values[-1], columns=selected_df.columns, index=None)\n",
    "            diff_df = pd.concat([subject_serie, finetune_serie, atlas_serie, diff_df], axis='columns')\n",
    "            diff_df.pop('index')\n",
    "            all_diff_df = all_diff_df.append(diff_df)\n",
    "    return all_diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1f85ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://github.com/hearbenchmark/hear2021-subm...\n",
       "1     https://github.com/hearbenchmark/hear2021-subm...\n",
       "2     https://github.com/hearbenchmark/hear2021-subm...\n",
       "3     https://github.com/hearbenchmark/hear2021-subm...\n",
       "4     https://github.com/hearbenchmark/hear2021-subm...\n",
       "5     https://github.com/hearbenchmark/hear2021-subm...\n",
       "6     https://github.com/hearbenchmark/hear2021-subm...\n",
       "7     https://github.com/hearbenchmark/hear2021-subm...\n",
       "8     https://github.com/hearbenchmark/hear2021-subm...\n",
       "9     https://github.com/hearbenchmark/hear2021-subm...\n",
       "10    https://github.com/hearbenchmark/hear2021-subm...\n",
       "11    https://github.com/hearbenchmark/hear2021-subm...\n",
       "12    https://github.com/hearbenchmark/hear2021-subm...\n",
       "13    https://github.com/hearbenchmark/hear2021-subm...\n",
       "14    https://github.com/hearbenchmark/hear2021-subm...\n",
       "15    https://github.com/hearbenchmark/hear2021-subm...\n",
       "16    https://github.com/hearbenchmark/hear2021-subm...\n",
       "17    https://github.com/hearbenchmark/hear2021-subm...\n",
       "18    https://github.com/hearbenchmark/hear2021-subm...\n",
       "19    https://github.com/hearbenchmark/hear2021-subm...\n",
       "20    https://github.com/hearbenchmark/hear2021-subm...\n",
       "21    https://github.com/hearbenchmark/hear2021-subm...\n",
       "22    https://github.com/hearbenchmark/hear2021-subm...\n",
       "23    https://github.com/hearbenchmark/hear2021-subm...\n",
       "24    https://github.com/hearbenchmark/hear2021-subm...\n",
       "25    https://github.com/hearbenchmark/hear2021-subm...\n",
       "26    https://github.com/hearbenchmark/hear2021-subm...\n",
       "27    https://github.com/hearbenchmark/hear2021-subm...\n",
       "28    https://github.com/hearbenchmark/hear2021-subm...\n",
       "Name: URL, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_236 = pd.read_csv(csv_paths_sub236)\n",
    "df_145 = pd.read_csv(csv_path_sub145)\n",
    "HEAREVAL_leaderboard = pd.read_csv(csv_path_leaderboard)\n",
    "HEAREVAL_leaderboard.pop('URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8179ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 35)\n"
     ]
    }
   ],
   "source": [
    "#dataframe with accuracies for all tests, for each subject\n",
    "HEAREVAL_df = pd.concat([df_236,df_145], ignore_index=True)\n",
    "HEAREVAL_df.sort_values(by=['subject', 'model'], inplace=True)\n",
    "HEAREVAL_df.drop('Unnamed: 0', axis='columns', inplace=True)\n",
    "models = HEAREVAL_df.pop('model')\n",
    "#HEAREVAL_df.set_index('model', inplace=True)\n",
    "acc_diff_df = diff_df(HEAREVAL_df, subs, atlas)\n",
    "print(acc_diff_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fca6562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n",
      "(36,) (1, 36)\n",
      "(37, 37)\n"
     ]
    }
   ],
   "source": [
    "#dataframe with ranks of all models in regards of other models from HEAREVAL benchmark\n",
    "\n",
    "eq = {\n",
    "    'model':'Model',\n",
    "    'beehive_states_fold0-v2-full_test_aucroc':'Beehive',\n",
    "    'beijing_opera-v1.0-hear2021-full_test_top1_acc_mean':'Beijing Opera',\n",
    "    'tfds_crema_d-1.0.0-full_test_top1_acc_mean':'CREMA-D',\n",
    "    'dcase2016_task2-hear2021-full_test_event_onset_200ms_fms':'DCASE 2016',\n",
    "    'esc50-v2.0.0-full_test_top1_acc_mean':'ESC-50',\n",
    "    'fsd50k-v1.0-full_test_mAP':'FSD50K',\n",
    "    'tfds_gtzan-1.0.0-full_test_top1_acc_mean':'GTZAN Genre',\n",
    "    'tfds_gtzan_music_speech-1.0.0-full_test_top1_acc_mean':'GTZAN Music/Speech',\n",
    "    'gunshot_triangulation-v1.0-full_test_top1_acc_mean':'Gunshot',\n",
    "    'libricount-v1.0.0-hear2021-full_test_top1_acc_mean':'Libricount',\n",
    "    'maestro-v3.0.0-5h_test_event_onset_50ms_fms_mean':'Maestro 5h',\n",
    "    'mridangam_stroke-v1.5-full_test_top1_acc_mean':'Mridangam Stroke',\n",
    "    'mridangam_tonic-v1.5-full_test_top1_acc_mean':'Mridangam Tonic',\n",
    "    'nsynth_pitch-v2.2.3-50h_test_pitch_acc':'NSynth Pitch 50h',\n",
    "    'nsynth_pitch-v2.2.3-5h_test_pitch_acc':'NSynth Pitch 5h',\n",
    "    'speech_commands-v0.0.2-5h_test_top1_acc':'Speech commands 5h',\n",
    "    'speech_commands-v0.0.2-full_test_top1_acc':'Speech commands full',\n",
    "    'vocal_imitation-v1.1.3-full_test_mAP_mean':'Vocal Imitation',\n",
    "    'vox_lingua_top10-hear2021-full_test_top1_acc_mean':'VoxLingua107 top 10' \n",
    "}\n",
    "\n",
    "renamed_df = pd.concat([models, HEAREVAL_df], axis='columns')\n",
    "renamed_df.rename(columns = eq, inplace=True)\n",
    "\n",
    "for sub in subs:\n",
    "    for a in atlas:\n",
    "        selected_df = renamed_df.loc[(renamed_df['subject'] == sub) & (renamed_df['atlas'] == a)]\n",
    "        baseline = selected_df.loc[selected_df['finetune'] == 'no']\n",
    "        for ft_model in ft_models:\n",
    "            print(model.shape, baseline.shape)\n",
    "            diff_baseline_model = renamed_df.loc[(renamed_df['subject'] == sub) & (renamed_df['atlas'] == a)]\n",
    "            print(diff_baseline_model.shape)\n",
    "            #merged_df = pd.merge(left=HEAREVAL_leaderboard, right=renamed_df, how='outer')\n",
    "            #ranked_df = merged_df.rank(numeric_only=True)\n",
    "            #print(ranked_df)\n",
    "            #models_hp = merged_df[['Model', 'subject','atlas','finetune']]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(left=HEAREVAL_leaderboard, right=renamed_df, how='outer')\n",
    "ranked_df = merged_df.rank(numeric_only=True)\n",
    "print(ranked_df)\n",
    "models_hp = merged_df[['Model', 'subject','atlas','finetune']]\n",
    "all_hp_ranked_df = pd.concat([models_hp, ranked_df], axis='columns')\n",
    "models_ranks_df = all_hp_ranked_df.loc[all_hp_ranked_df['subject'].notna()]\n",
    "print(models_ranks_df)\n",
    "models = models_ranks_df.pop('Model')\n",
    "rank_diff_df = diff_df(models_ranks_df, subs, atlas)\n",
    "print(rank_diff_df.shape)\n",
    "print(rank_diff_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
