{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cad9254-df87-40ee-8d7c-831ded610c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "#check if 1.7 (old statistic) or 1.11 (new statistic)\n",
    "import scipy\n",
    "print(scipy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41660759-e3ad-4219-866d-0a1d4fd8b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile HEAR eval results from json\n",
    "path = '/home/maellef/Results/finefriends/group_model/HEAReval'\n",
    "pair_task_test = '/home/maellef/git/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/task-test.csv'\n",
    "\n",
    "task_test_df = pd.read_csv(pair_task_test, sep=',')\n",
    "group_metrics_df = pd.DataFrame()\n",
    "\n",
    "for dir in os.listdir(path):\n",
    "    atlas = 'STG' if 'auditory_Voxels' in dir else 'wholebrain'\n",
    "    sub_dict = {\n",
    "        'model':'group',\n",
    "        'finetune':'conv4',\n",
    "        'subject':dir[:6],\n",
    "        'atlas':atlas\n",
    "               }\n",
    "    \n",
    "    model_path = os.path.join(path, dir, 'soundnetbrain_hear')\n",
    "    for i, benchmark in enumerate(os.listdir(model_path)):\n",
    "        test = task_test_df['test'].loc[task_test_df['task']==benchmark].values[0]\n",
    "        benchmark_score_path = os.path.join(model_path, benchmark, 'test.predicted-scores.json')\n",
    "        with open(benchmark_score_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            if 'test' in data.keys():\n",
    "                scores = data['test']\n",
    "            elif \"aggregated_scores\" in data.keys():\n",
    "                scores = data['aggregated_scores']\n",
    "            selected_score = scores[test]\n",
    "            test_name = f'{benchmark}_{test}'\n",
    "            sub_dict[test_name] = selected_score\n",
    "\n",
    "    sub_df = pd.DataFrame([sub_dict])\n",
    "    group_metrics_df = pd.concat([group_metrics_df, sub_df], ignore_index=True)\n",
    "\n",
    "group_metrics_df.to_csv('/home/maellef/git/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/metrics_group.csv', sep=',')\n",
    "print(group_metrics_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649eb4c2-c161-4c3b-aad5-0921f0d94cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths + global variables\n",
    "subs = ['sub-01','sub-02', 'sub-03','sub-04', 'sub-05', 'sub-06']\n",
    "ft_models = ['conv4', 'conv5', 'conv6', 'conv7', 'no']\n",
    "scales = ['wholebrain', 'STG']\n",
    "baseline = 'no'\n",
    "path_to_repo = '/home/maellef/git' #'/home/maelle/GitHub_repositories'\n",
    "\n",
    "csv_path_sub236 = path_to_repo + '/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/metrics.csv'\n",
    "csv_path_sub145 = path_to_repo + '/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/metrics_145.csv'\n",
    "csv_path_group = path_to_repo + '/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/metrics_group.csv'\n",
    "csv_path_leaderboard = path_to_repo + '/cNeuromod_encoding_2020/benchmark/HEAR-EVAL/leaderboard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733b4d5a-92d2-413f-8348-e82cf157af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge & format all models results from HEAREVAL\n",
    "eq = {\n",
    "    'model':'Model',\n",
    "    'beehive_states_fold0-v2-full_test_aucroc':'Beehive',\n",
    "    'beehive_states_fold1-v2-full_test_aucroc':'Beehive_fold1',\n",
    "    'beijing_opera-v1.0-hear2021-full_test_top1_acc_mean':'Beijing Opera',\n",
    "    'beijing_opera-v1.0-hear2021-full_test_top1_acc_std':'Beijing Opera_std',\n",
    "    'tfds_crema_d-1.0.0-full_test_top1_acc_mean':'CREMA-D',\n",
    "    'tfds_crema_d-1.0.0-full_test_top1_acc_std':'CREMA-D_std',\n",
    "    'dcase2016_task2-hear2021-full_test_event_onset_200ms_fms':'DCASE 2016',\n",
    "    'esc50-v2.0.0-full_test_top1_acc_mean':'ESC-50',\n",
    "    'esc50-v2.0.0-full_test_top1_acc_std':'ESC-50_std',\n",
    "    'fsd50k-v1.0-full_test_mAP':'FSD50K',\n",
    "    'tfds_gtzan-1.0.0-full_test_top1_acc_mean':'GTZAN Genre',\n",
    "    'tfds_gtzan-1.0.0-full_test_top1_acc_std' : 'GTZAN Genre_std',    \n",
    "    'tfds_gtzan_music_speech-1.0.0-full_test_top1_acc_mean':'GTZAN Music/Speech',\n",
    "    'tfds_gtzan_music_speech-1.0.0-full_test_top1_acc_std' :'GTZAN Music/Speech_std',\n",
    "    'gunshot_triangulation-v1.0-full_test_top1_acc_mean':'Gunshot',\n",
    "    'gunshot_triangulation-v1.0-full_test_top1_acc_std':'Gunshot_std',\n",
    "    'libricount-v1.0.0-hear2021-full_test_top1_acc_mean':'Libricount',\n",
    "    'libricount-v1.0.0-hear2021-full_test_top1_acc_std':'Libricount_std',\n",
    "    'maestro-v3.0.0-5h_test_event_onset_50ms_fms_mean':'Maestro 5h',\n",
    "    'maestro-v3.0.0-5h_test_event_onset_50ms_fms_std':'Maestro 5h_std',\n",
    "    'mridangam_stroke-v1.5-full_test_top1_acc_mean':'Mridangam Stroke',\n",
    "    'mridangam_stroke-v1.5-full_test_top1_acc_std':'Mridangam Stroke_std',\n",
    "    'mridangam_tonic-v1.5-full_test_top1_acc_mean':'Mridangam Tonic',\n",
    "    'mridangam_tonic-v1.5-full_test_top1_acc_std':'Mridangam Tonic_std',\n",
    "    'nsynth_pitch-v2.2.3-50h_test_pitch_acc':'NSynth Pitch 50h',\n",
    "    'nsynth_pitch-v2.2.3-5h_test_pitch_acc':'NSynth Pitch 5h',\n",
    "    'speech_commands-v0.0.2-5h_test_top1_acc':'Speech commands 5h',\n",
    "    'speech_commands-v0.0.2-full_test_top1_acc':'Speech commands full',\n",
    "    'vocal_imitation-v1.1.3-full_test_mAP_mean':'Vocal Imitation',\n",
    "    'vocal_imitation-v1.1.3-full_test_mAP_std':'Vocal Imitation_std',\n",
    "    'vox_lingua_top10-hear2021-full_test_top1_acc_mean':'VoxLingua107 top 10', \n",
    "    'vox_lingua_top10-hear2021-full_test_top1_acc_std':'VoxLingua107 top 10_std'\n",
    "}\n",
    "\n",
    "df_236 = pd.read_csv(csv_path_sub236)\n",
    "df_145 = pd.read_csv(csv_path_sub145)\n",
    "df_group = pd.read_csv(csv_path_group)\n",
    "HEAREVAL_df = pd.concat([df_236,df_145, df_group], ignore_index=True)\n",
    "HEAREVAL_df.sort_values(by=['subject', 'model'], inplace=True)\n",
    "HEAREVAL_df.drop('Unnamed: 0', axis='columns', inplace=True)\n",
    "HEAREVAL_df.rename(columns = eq, inplace=True)\n",
    "models = HEAREVAL_df['Model'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd4f488-8212-46e7-8110-96d4970d3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide results from HEAREVAL models in 2 catgerories : small models VS big models \n",
    "small_models=[\n",
    "    'OpenL3',\n",
    "    'Descript/MARL Wav2CLIP',\n",
    "    'IUT-CSE MLP (audio)',\n",
    "    'IUT-CSE MLP (keyword)',\n",
    "    'Logitech AI SERAB BYOL-S',\n",
    "    'RedRice EfficientNet-B2',\n",
    "    'Sony UDONS ViT',\n",
    "    'Soundsensing YAMNet'\n",
    "]\n",
    "HEAREVAL_leaderboard_full = pd.read_csv(csv_path_leaderboard)\n",
    "HEAREVAL_leaderboard_full.pop('URL')\n",
    "HEAREVAL_leaderboard_small = HEAREVAL_leaderboard_full[HEAREVAL_leaderboard_full['Model'].isin(small_models)]\n",
    "HEAREVAL_leaderboard_big = HEAREVAL_leaderboard_full[~HEAREVAL_leaderboard_full['Model'].isin(small_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662b79ce-d06a-4b48-8c1b-183ef8c84cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide tasks by their training dataset size\n",
    "tasks_training_size = {\n",
    "    'Gunshot':114,\n",
    "    'Beijing Opera':902,\n",
    "    'GTZAN Music/Speech':3456,\n",
    "    'Mridangam Stroke':4521,\n",
    "    'Mridangam Tonic':4521,\n",
    "    'DCASE 2016':6912,\n",
    "    'ESC-50':8000,\n",
    "    'VoxLingua107 top 10':14494,\n",
    "    'NSynth Pitch 5h':16000,\n",
    "    'Maestro 5h':17760,\n",
    "    'Speech commands 5h':18312,\n",
    "    'Libricount':22880,\n",
    "    'GTZAN Genre':27000,\n",
    "    'CREMA-D':29755,\n",
    "    'Vocal Imitation':42044,\n",
    "    'Speech commands full':80402,\n",
    "    'NSynth Pitch 50h':156992,\n",
    "    'Beehive':276480,\n",
    "    'FSD50K':289440\n",
    "}\n",
    "tasks = list(tasks_training_size.keys())\n",
    "sizes = list(tasks_training_size.values())\n",
    "tasks_training = {'task':tasks,\n",
    "                 'size(s)':sizes}\n",
    "\n",
    "task_training_df = pd.DataFrame(data = tasks_training)\n",
    "task_training_df['size(min)']=task_training_df['size(s)']/60\n",
    "task_training_df['size(hour)']=task_training_df['size(s)']/60/60\n",
    "task_training_small = task_training_df.loc[task_training_df['size(min)']<90]\n",
    "task_training_medium = task_training_df.loc[(task_training_df['size(min)']>90)&(task_training_df['size(min)']<600)]\n",
    "task_training_big = task_training_df.loc[task_training_df['size(min)']>600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68237f0-a746-4541-a89e-5c0d68e5b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for ranking :\n",
    "def rank_models_amongst_HEAREVAL(models, small=False, big=False):\n",
    "    if small & big:\n",
    "        HEAREVAL_leaderboard = HEAREVAL_leaderboard_full\n",
    "    elif big:\n",
    "        HEAREVAL_leaderboard = HEAREVAL_leaderboard_big\n",
    "    else: \n",
    "        HEAREVAL_leaderboard = HEAREVAL_leaderboard_small\n",
    "        \n",
    "    merged_df = pd.merge(left=HEAREVAL_leaderboard, right=models, how='outer')\n",
    "    models = merged_df['Model']\n",
    "    ranked_df = merged_df.rank(numeric_only=False, ascending=False)\n",
    "    ranked_df['Model'] = models\n",
    "    return ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992bef8d-432c-44ea-89d6-e7d0ef314323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the different soundnet baseline that can be used \n",
    "soundnet_HE_results = HEAREVAL_df[HEAREVAL_df['finetune'] == 'no']\n",
    "soundnet_mean_results = soundnet_HE_results.mean(numeric_only=True)\n",
    "soundnet_median_results = soundnet_HE_results.median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378eb30c-76bf-4cd9-a248-4ed3073ef3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEAR EVAL Task scores for every model\n",
    "#atlas = 'STG' #'wholebrain' \n",
    "\n",
    "soundnet_model = soundnet_median_results.to_frame().T\n",
    "soundnet_model['Model'] = 'soundnet'\n",
    "others_models = HEAREVAL_leaderboard_full\n",
    "finetuned_models = HEAREVAL_df.loc[(HEAREVAL_df['finetune']=='conv4')].copy() #& (HEAREVAL_df['atlas']==atlas)\n",
    "for i, row in finetuned_models.iterrows():\n",
    "    model_name = f'{row[\"subject\"]}_{row[\"finetune\"]}_{row[\"atlas\"]}'\n",
    "    finetuned_models.at[i, 'Model'] = model_name+'_group' if row['Model']=='group' else model_name\n",
    "finetuned_models.drop(['atlas', 'finetune'], axis='columns', inplace=True)\n",
    "\n",
    "all_models = pd.concat([others_models, finetuned_models, soundnet_model], axis='rows', join='inner')\n",
    "HEAREVAL_tasks = pd.DataFrame({'nb_models': all_models.count(), \n",
    "                               'min':all_models.min(), \n",
    "                               'max':all_models.max(),\n",
    "                               'median':all_models.median(numeric_only=True)})\n",
    "HEAREVAL_tasks['Soundnet'] = soundnet_model.T\n",
    "HEAREVAL_tasks.drop('Model', axis='rows', inplace=True)\n",
    "\n",
    "for i, row in finetuned_models.iterrows():\n",
    "    col_name = row['Model']\n",
    "    diff_name = 'diff_'+row['Model']\n",
    "    \n",
    "    HEAREVAL_tasks[col_name] = row\n",
    "    HEAREVAL_tasks[diff_name] = row-HEAREVAL_tasks['Soundnet']\n",
    "\n",
    "type_test = {\n",
    "    'Beehive':'aucroc',\n",
    "    'Beijing Opera':'top1_acc',\n",
    "    'CREMA-D':'top1_acc',\n",
    "    'DCASE 2016':'event_onset_200ms',\n",
    "    'ESC-50':'top1_acc',\n",
    "    'FSD50K':'mAP',\n",
    "    'GTZAN Genre':'top1_acc',\n",
    "    'GTZAN Music/Speech':'top1_acc',\n",
    "    'Gunshot':'top1_acc',\n",
    "    'Libricount':'top1_acc',\n",
    "    'Maestro 5h':'event_onset_50ms',\n",
    "    'Mridangam Stroke':'top1_acc',\n",
    "    'Mridangam Tonic':'top1_acc',\n",
    "    'NSynth Pitch 5h':'pitch_acc',\n",
    "    'NSynth Pitch 50h':'pitch_acc',\n",
    "    'Speech commands 5h':'top1_acc',\n",
    "    'Speech commands full':'top1_acc',\n",
    "    'Vocal Imitation':'mAP',\n",
    "    'VoxLingua107 top 10':'top1_acc', \n",
    "}\n",
    "test_df = pd.DataFrame(type_test.values(), index = type_test.keys())\n",
    "HEAREVAL_tasks['test'] = test_df\n",
    "print(HEAREVAL_tasks)\n",
    "HEAREVAL_tasks.to_csv('/home/maellef/Results/finefriends/figures/figure_papier/HEAREVAL_tasks_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5475749c-68fe-4ae5-914c-fdbc29953ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beehive 0.07379257678985596 False\n",
      "Beijing Opera 1.6689300537109375e-06 True\n",
      "CREMA-D 1.633167266845703e-05 True\n",
      "DCASE 2016 0.0035042762756347656 True\n",
      "ESC-50 5.960464477539062e-07 True\n",
      "FSD50K 2.384185791015625e-07 True\n",
      "GTZAN Genre 1.1920928955078125e-07 True\n",
      "GTZAN Music/Speech 0.04248917102813721 True\n",
      "Gunshot 1.6689300537109375e-06 True\n",
      "Libricount 1.1920928955078125e-07 True\n",
      "Maestro 5h 0.027281171477617997 True\n",
      "Mridangam Stroke 0.7682963609695435 False\n",
      "Mridangam Tonic nan False\n",
      "NSynth Pitch 50h 1.1920928955078125e-07 True\n",
      "NSynth Pitch 5h 0.00027811527252197266 True\n",
      "Speech commands 5h 0.0008462667465209961 True\n",
      "Speech commands full 1.633167266845703e-05 True\n",
      "Vocal Imitation nan False\n",
      "VoxLingua107 top 10 0.005897641181945801 True\n",
      "number of tasks with significantly improved performance: 12/19, decreased performance: 2/19, ['DCASE 2016', 'VoxLingua107 top 10'], no change: 5/19, ['Beehive', 'Maestro 5h', 'Mridangam Stroke', 'Mridangam Tonic', 'Vocal Imitation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "HEAR_tasks = pd.read_csv('/home/maellef/Results/finefriends/figures/figure_papier/HEAREVAL_tasks_data.csv', index_col=0)\n",
    "\n",
    "#atlas = 'STG' #'wholebrain'\n",
    "include_group = True\n",
    "include_indvd = True\n",
    "\n",
    "all_diff = HEAR_tasks.filter(like='diff', axis=1)\n",
    "#all_diff = all_diff.filter(like=atlas, axis=1)\n",
    "if include_group and not include_indvd:\n",
    "    all_diff = all_diff.filter(like='group', axis=1)\n",
    "elif include_indvd and not include_group:\n",
    "    temp = all_diff.filter(regex=\"group\", axis=1)\n",
    "    all_diff.drop(labels=list(temp.columns), axis='columns', inplace=True)\n",
    "\n",
    "diffpos, diffneg, same = 0, 0, 0\n",
    "negtasks, nosigtasks = [], []\n",
    "for task, row in all_diff.iterrows():\n",
    "    stat, pvalue = wilcoxon(row.values, nan_policy='propagate')\n",
    "    print(task, pvalue, pvalue<0.05)\n",
    "    #print(row.values)\n",
    "    if pvalue<0.05 and np.median(row.values) > 0:\n",
    "        diffpos+=1\n",
    "    elif pvalue<0.05 and np.median(row.values) < 0:\n",
    "        diffneg+=1\n",
    "        negtasks.append(task)\n",
    "    else:\n",
    "        same+=1\n",
    "        nosigtasks.append(task)\n",
    "print(f'number of tasks with significantly improved performance: {diffpos}/19, decreased performance: {diffneg}/19, {negtasks}, no change: {same}/19, {nosigtasks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eeec2d-3445-4f7b-9d87-f78ac8c7a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAR EVAL Scatterplot\n",
    "all_models2 = all_models.copy()\n",
    "subject, scale = [], []\n",
    "for i, row in all_models2.iterrows():\n",
    "    if row['Model'] == 'soundnet':\n",
    "        model_sub = 'Soundnet'\n",
    "        model_scale = 'Soundnet'\n",
    "        \n",
    "    elif 'sub-' in row['Model']:\n",
    "        model_sub = row['Model'][:6]\n",
    "        model_scale = 'STG' if 'STG' in row['Model'] else 'Whole Brain'\n",
    "        model_scale = model_scale+' group' if 'group' in row['Model'] else model_scale\n",
    "    else:\n",
    "        if row['Model'] in small_models:\n",
    "            model_scale = 'HEAR EVAL small' \n",
    "        elif row['Model'] in list(HEAREVAL_leaderboard_big['Model']):\n",
    "            model_scale = 'HEAR EVAL big'\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    scale.append(model_scale)\n",
    "    subject.append(model_sub)\n",
    "\n",
    "all_models2['subject'] = subject\n",
    "all_models2['scale'] = scale\n",
    "\n",
    "df_graph = all_models2.melt(id_vars=['Model','subject', 'scale'], var_name = 'task', value_name = 'result')\n",
    "print(df_graph)\n",
    "\n",
    "models_size = 'both'\n",
    "if models_size == 'small':\n",
    "    index = df_graph.loc[df_graph['subject']=='HEAR EVAL big'].index\n",
    "elif models_size == 'big':\n",
    "    index = df_graph.loc[df_graph['subject']=='HEAR EVAL small'].index\n",
    "else:\n",
    "    index = []\n",
    "df_graph.drop(index, inplace=True)\n",
    "\n",
    "#color = 'teal' if atlas =='STG' else 'indigo'\n",
    "#edgecolor = 'w' if atlas =='STG' else 'gray'\n",
    "#color_big = 'w' if atlas =='STG' else 'slategray'\n",
    "#color_small = 'w' if atlas =='STG' else 'lightsteelblue'\n",
    "    \n",
    "palette = {'HEAR EVAL big':'dimgray',\n",
    "           'HEAR EVAL small':'darkgray',\n",
    "           'STG':'teal',\n",
    "           'STG group':'turquoise',\n",
    "           'Whole Brain':'indigo',\n",
    "           'Whole Brain group':'mediumpurple',\n",
    "           'Soundnet':'firebrick',\n",
    "          }\n",
    "hue_order = ['HEAR EVAL big', 'HEAR EVAL small', 'STG', 'STG group', 'Whole Brain', 'Whole Brain group', 'Soundnet']\n",
    "#index = df_graph.loc[df_graph['subject']=='HEAR EVAL small'].index\n",
    "#index2 = df_graph.loc[df_graph['subject']=='HEAR EVAL big'].index\n",
    "#all_i = np.concatenate((index,index2))\n",
    "#df_graph.drop(all_i, inplace=True)\n",
    "\n",
    "plt.subplots(figsize=(35, 20))\n",
    "ax = sns.boxplot(data=df_graph, x=\"task\", y=\"result\", hue=\"scale\", hue_order=hue_order, dodge=True,\n",
    "                   order=tasks, palette=palette, linewidth=0.2, medianprops={\"color\": \"w\", \"linewidth\": 0.5}) #edgecolor='w'\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(1.2,1))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.yticks(rotation = 90)\n",
    "ax.set(ylabel=\"results for {} models\".format(models_size))\n",
    "ax.set(xlabel=\"\")\n",
    "plt.savefig('/home/maellef/Results/finefriends/figures/figure_papier/HEAR_EVAL_{}_models_with_group_boxplot.png'.format(models_size), \n",
    "            bbox_inches='tight', transparent=False, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115d125-a0cd-42f3-8a83-913c7ba0fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confidence interval for soundnet model : score/rank\n",
    "\n",
    "soundnet_ranks = pd.DataFrame()\n",
    "for i, row in soundnet_HE_results.iterrows():\n",
    "    row = row.to_frame().T\n",
    "    all_models_ranked_df = rank_models_amongst_HEAREVAL(models = row)\n",
    "    soundnet_model = all_models_ranked_df[-1:]\n",
    "    soundnet_ranks = pd.concat([soundnet_ranks, soundnet_model])\n",
    "\n",
    "soundnet_ranks= soundnet_ranks.set_index(keys='Model')\n",
    "soundnet_ranks.drop(list(soundnet_ranks.filter(regex='_std|_fold1')), axis='columns', inplace=True)\n",
    "soundnet_ranks.drop(['subject', 'finetune', 'atlas'], axis='columns', inplace=True)\n",
    "a = soundnet_ranks.describe().T.drop('count', axis=1)\n",
    "\n",
    "a['75-25'] = a['75%'] - a['25%']\n",
    "\n",
    "outliers_nb = []\n",
    "all_ranks = []\n",
    "for test in soundnet_ranks.columns:\n",
    "    test_results = soundnet_ranks[test].values\n",
    "    all_ranks.append(test_results)\n",
    "    low_threshold = a.loc[test]['25%']\n",
    "    high_threshold = a.loc[test]['75%']\n",
    "    test_outliers = ((test_results<low_threshold) | (test_results > high_threshold)).sum()\n",
    "    outliers_nb.append(test_outliers)\n",
    "    \n",
    "a['outliers_nb'] = outliers_nb\n",
    "a['ranks_values'] = all_ranks\n",
    "print(a)\n",
    "a.to_csv('/home/maellef/Results/figures/figure_papier/Soundnet_data.csv')\n",
    "#a = soundnet_ranks.describe()\n",
    "#print(a.loc['75%'].sub(a.loc['25%']))\n",
    "#print(soundnet_ranks[['Model', 'Beijing Opera', 'Gunshot', 'Mridangam Stroke', 'Mridangam Tonic', 'NSynth Pitch 5h']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51773ed6-c4fa-484a-a3ec-715383633f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundnet_baseline = soundnet_median_results.to_frame().T\n",
    "soundnet_baseline['Model'] = 'soundnet_median'\n",
    "selected_finetune = 'conv4'\n",
    "all_df = pd.DataFrame()\n",
    "small = True\n",
    "big = True\n",
    "solo_soundnet = False\n",
    "\n",
    "for sub in subs:\n",
    "    for scale in scales:\n",
    "        for group in [False, True]:\n",
    "            #1 - model selection & baseline addition\n",
    "            conditions_list = [(HEAREVAL_df['subject'] == sub),\n",
    "                               (HEAREVAL_df['atlas'] == scale), \n",
    "                               (HEAREVAL_df['finetune'] == selected_finetune)]\n",
    "            if group:\n",
    "                conditions_list.append((HEAREVAL_df['Model'] == 'group'))\n",
    "            else:\n",
    "                conditions_list.append((HEAREVAL_df['Model'] != 'group'))\n",
    "    \n",
    "            full_cdt = True\n",
    "            for condition in conditions_list:\n",
    "                full_cdt &= condition     \n",
    "            selected_df = HEAREVAL_df.loc[full_cdt]\n",
    "            \n",
    "            model_hp = selected_df[['Model', 'subject','atlas','finetune']].set_index(keys='Model')\n",
    "            model_hp['training_data'] = 'group' if group else 'individual'\n",
    "            if not solo_soundnet:\n",
    "                #Version 1 - rank both soundnet and finetune at the same time\n",
    "                selected_df = pd.concat([selected_df, soundnet_baseline]).drop(labels=['subject','atlas','finetune'], axis='columns')\n",
    "            else:\n",
    "                #version 2 - rank each model individually\n",
    "                selected_df = selected_df.drop(labels=['subject','atlas','finetune'], axis='columns')\n",
    "                \n",
    "            #2 - ranking of selected model + baseline amongst HEAREVAL models\n",
    "            all_models_ranked_df = rank_models_amongst_HEAREVAL(models = selected_df, small=small, big=big)\n",
    "            if solo_soundnet:\n",
    "                soundnet_rank_df = rank_models_amongst_HEAREVAL(models = soundnet_baseline, small=small, big=big)\n",
    "            \n",
    "            if not solo_soundnet:\n",
    "                finetuned_ranked_df = all_models_ranked_df[-2:].set_index(keys='Model')\n",
    "            else:\n",
    "                finetuned_ranked_df = all_models_ranked_df[-1:].set_index(keys='Model')\n",
    "                soundnet_rank_df = soundnet_rank_df[-1:].set_index(keys='Model')\n",
    "                finetuned_ranked_df = pd.concat([finetuned_ranked_df, soundnet_rank_df])\n",
    "            \n",
    "            ranked_diff = finetuned_ranked_df.diff(periods=-1).mul(-1)\n",
    "            \n",
    "            #3 - melt all test columns into one column 'test' + add both rank and diff rank columns\n",
    "            a = []\n",
    "            if sub == 'sub-01':\n",
    "                soundnet_median_ranks = finetuned_ranked_df.loc['soundnet_median'].to_frame().T\n",
    "                \n",
    "            ft_ranked = finetuned_ranked_df.iloc[0].to_frame().T\n",
    "            ft_ranked_diff = ranked_diff.iloc[0].to_frame().T\n",
    "            for result, df in zip(('rank', 'diff_rank'), [ft_ranked, ft_ranked_diff]):\n",
    "                model_diff = pd.concat([model_hp, df], axis='columns', join='inner')\n",
    "                model_diff.drop(list(model_diff.filter(regex='_std|_fold1')), axis='columns', inplace=True)\n",
    "                model_melt = model_diff.melt(id_vars=['subject','atlas','finetune', 'training_data'], \n",
    "                                                   var_name = 'test', value_name = result)\n",
    "                a.append(model_melt)\n",
    "                \n",
    "            #merge all df for visualisation\n",
    "            sub_results_df = pd.merge(a[0], a[1], how = 'inner')   \n",
    "            all_df = pd.concat([all_df, sub_results_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72e81cbf-a0a9-4c21-9b8f-772c34f74a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-01 wholebrain -0.00010423713589790339 0.7771175732880233 False\n",
      "sub-02 wholebrain -0.004733746158422963 0.35197884167136195 False\n",
      "sub-03 wholebrain 0.0021345388053102842 0.8647045760375567 False\n",
      "sub-04 wholebrain 0.004415382073170103 0.060207366943359375 False\n",
      "sub-05 wholebrain 0.010610329711872995 0.4413337707519531 False\n",
      "sub-06 wholebrain -0.0003287551422034214 0.4037628173828125 False\n",
      "sub-01 STG -0.009115566916777288 0.001176563999937216 True\n",
      "sub-02 STG -6.127998180037207e-05 0.7173808884143212 False\n",
      "sub-03 STG -0.0029346280492379135 0.3778228759765625 False\n",
      "sub-04 STG -0.00667588016093364 0.28597919066405664 False\n",
      "sub-05 STG -0.013841588133978044 0.001176563999937216 True\n",
      "sub-06 STG -0.007775743340593757 0.46911315243545415 False\n",
      "-0.002372282807529862 0.03917421676388495 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/maellef/env/Py310_finefriends/lib/python3.10/site-packages/scipy/stats/_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n"
     ]
    }
   ],
   "source": [
    "HEAR_tasks = pd.read_csv('/home/maellef/Results/finefriends/figures/figure_papier/HEAREVAL_tasks_data.csv', index_col=0)\n",
    "all_dif_score = []\n",
    "for scale in scales:\n",
    "    for sub in subs:\n",
    "        sub_scores_diff = HEAR_tasks.filter(regex=f'^{sub}').filter(regex=scale).diff(axis=1)\n",
    "        diff_val = sub_scores_diff.iloc[:,-1].values\n",
    "        all_dif_score.extend(diff_val)\n",
    "        _, pvalue = wilcoxon(diff_val, nan_policy='omit')\n",
    "        print(sub, scale, np.nanmean(diff_val), pvalue, pvalue<0.05 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "204023ff-731f-4a69-9483-d8cb103c517b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-01 wholebrain -0.8947368421052632 mean group:  1.1578947368421053 , mean ind:  2.0526315789473686 0.3844120797513729 False\n",
      "sub-02 wholebrain 1.088235294117647 mean group:  2.236842105263158 , mean ind:  1.2941176470588236 0.0649777819760697 False\n",
      "sub-03 wholebrain 0.0 mean group:  2.0526315789473686 , mean ind:  2.235294117647059 0.5486660989062389 False\n",
      "sub-04 wholebrain -0.9473684210526315 mean group:  1.868421052631579 , mean ind:  2.8157894736842106 0.018036975635797568 True\n",
      "sub-05 wholebrain -0.4473684210526316 mean group:  1.6842105263157894 , mean ind:  2.1315789473684212 0.9719968677847196 False\n",
      "sub-06 wholebrain -0.9705882352941176 mean group:  1.763157894736842 , mean ind:  2.8823529411764706 0.37173026702997414 False\n",
      "wholebrain -0.38425925925925924 mean group:  1.793859649122807 , mean ind:  2.240740740740741 0.49927819105799587 False\n",
      "sub-01 STG 1.105263157894737 mean group:  2.8421052631578947 , mean ind:  1.736842105263158 0.005118643800037987 True\n",
      "sub-02 STG 0.35294117647058826 mean group:  2.4210526315789473 , mean ind:  2.235294117647059 0.9155825203905921 False\n",
      "sub-03 STG -0.6176470588235294 mean group:  1.894736842105263 , mean ind:  2.6176470588235294 0.4392541254123927 False\n",
      "sub-04 STG 0.9473684210526315 mean group:  2.526315789473684 , mean ind:  1.5789473684210527 0.13034705624546747 False\n",
      "sub-05 STG 1.5789473684210527 mean group:  2.5789473684210527 , mean ind:  1.0 0.007292038706980377 True\n",
      "sub-06 STG 1.588235294117647 mean group:  2.3157894736842106 , mean ind:  0.8823529411764706 0.12164285939134006 False\n",
      "STG 0.8472222222222222 mean group:  2.4298245614035086 , mean ind:  1.662037037037037 0.004176220459889514 True\n",
      "0.23148148148148148 mean group:  2.111842105263158 , mean ind:  1.9513888888888888 0.12763312335614055 False\n",
      "mean wholebrain:  2.0112612612612613 , stg:  2.0563063063063063 -0.04504504504504504 0.9738900547989003 False\n"
     ]
    }
   ],
   "source": [
    "tasks = list(tasks_training_size.keys())\n",
    "all_diff_group_ind, all_group, all_ind = [], [], []\n",
    "both_scale = []\n",
    "for scale in scales:\n",
    "    scale_diff, scale_group, scale_ind = [], [], []\n",
    "    for sub in subs:\n",
    "        sub_diff, sub_ind, sub_group = [], [], []\n",
    "        for task in tasks:\n",
    "            df_task = all_df.loc[(all_df['subject']==sub)\n",
    "                                & (all_df['atlas']==scale)\n",
    "                                & (all_df['test']==task)]\n",
    "            ranks = df_task['diff_rank'].values\n",
    "            sub_ind.append(ranks[0])\n",
    "            sub_group.append(ranks[1])\n",
    "            scale_ind.append(ranks[0])\n",
    "            scale_group.append(ranks[1])\n",
    "            all_ind.append(ranks[0])\n",
    "            all_group.append(ranks[1])\n",
    "            \n",
    "            diff_group_ind = ranks[1] - ranks[0]\n",
    "            all_diff_group_ind.append(diff_group_ind)\n",
    "            sub_diff.append(diff_group_ind)\n",
    "            scale_diff.append(diff_group_ind)\n",
    "            \n",
    "        _, pvalue = wilcoxon(sub_diff, nan_policy='omit')\n",
    "        print(sub, scale, np.nanmean(sub_diff), 'mean group: ', np.nanmean(sub_group),', mean ind: ', np.nanmean(sub_ind), pvalue, pvalue<0.05, )\n",
    "    _, pvalue = wilcoxon(scale_diff, nan_policy='omit')\n",
    "    print(scale, np.nanmean(scale_diff), 'mean group: ', np.nanmean(scale_group),', mean ind: ', np.nanmean(scale_ind), pvalue, pvalue<0.05,)\n",
    "    both_scale.append(scale_ind+scale_group)\n",
    "_, pvalue = wilcoxon(all_diff_group_ind, nan_policy='omit')\n",
    "print(np.nanmean(all_diff_group_ind), 'mean group: ', np.nanmean(all_group),', mean ind: ', np.nanmean(all_ind), pvalue, pvalue<0.05)\n",
    "\n",
    "diff_scale = np.array(both_scale[0])-np.array(both_scale[1])\n",
    "_, pvalue = wilcoxon(diff_scale, nan_policy='omit')\n",
    "print('mean wholebrain: ', np.nanmean(both_scale[0]), ', stg: ', np.nanmean(both_scale[1]), np.nanmean(diff_scale), pvalue, pvalue<0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e62cf-7283-4d4d-aede-16cd6f41ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soundnet_median_ranks)\n",
    "soundnet_median_ranks['subject'] = 'Soundnet'\n",
    "soundnet_median_ranks.drop(list(soundnet_median_ranks.filter(regex='_std|_fold1')), axis='columns', inplace=True)\n",
    "soundnet_median_ranks = soundnet_median_ranks.melt(id_vars=['subject'], \n",
    "                                               var_name = 'test', value_name = 'rank')\n",
    "all_df = pd.concat([all_df, soundnet_median_ranks], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081d82f-7fdd-4db3-ac9e-e36d67691826",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83487d49-3876-45e9-a72d-0195f6a3d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stat\n",
    "conv='conv4'\n",
    "for scale in scales : \n",
    "    for sub in subs : \n",
    "        df_sub = all_df.loc[(all_df['finetune'] == conv) & (all_df['atlas'] == scale) & (all_df['subject'] == sub)]\n",
    "        diff_values = df_sub['diff_rank'].values\n",
    "        stat, pvalue = wilcoxon(diff_values, nan_policy='omit')\n",
    "        print(conv, scale, sub, pvalue, pvalue<0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75102226-d598-4670-b442-0c2b2c976fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f30bc-79f6-46be-8f30-e9fa24d15cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = 'STG' #'wholebrain'\n",
    "df_wip = all_df.loc[all_df['atlas']==atlas]\n",
    "df_wip.drop(['atlas', 'finetune'], axis='columns', inplace=True)\n",
    "df_wip = df_wip.melt(id_vars=['subject','test', 'training_data'], var_name = 'type', value_name = 'result')\n",
    "df_wip = df_wip.pivot(index=\"test\", columns=[\"subject\", 'training_data', \"type\"], values=\"result\")\n",
    "columns_order = [('sub-01', 'rank'),('sub-01', 'diff_rank'),\n",
    "                 ('sub-02', 'rank'),('sub-02', 'diff_rank'),\n",
    "                 ('sub-03', 'rank'),('sub-03', 'diff_rank'),\n",
    "                 ('sub-04', 'rank'),('sub-04', 'diff_rank'),\n",
    "                 ('sub-05', 'rank'),('sub-05', 'diff_rank'),\n",
    "                 ('sub-06', 'rank'),('sub-06', 'diff_rank')]\n",
    "\n",
    "rows_order = ['Gunshot','Beijing Opera','NSynth Pitch 5h','NSynth Pitch 50h','ESC-50',\n",
    "              'Speech commands 5h','CREMA-D','FSD50K','GTZAN Genre','GTZAN Music/Speech',\n",
    "              'Speech commands full','Mridangam Stroke','Libricount','Maestro 5h',\n",
    "              'Beehive','VoxLingua107 top 10','DCASE 2016','Vocal Imitation','Mridangam Tonic']\n",
    "df_wip = df_wip.reindex(rows_order)\n",
    "df_wip.to_csv(f'/home/maellef/Results/finefriends/figures/figure_papier/HEAREVAL_full_rank_data_with_group_{atlas}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c9fe8-b00f-449b-9708-58f7f12583ce",
   "metadata": {},
   "source": [
    "test full benchmark model new scale legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97657885-c6fa-42b6-a617-f9f34a034bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60040244-8572-48aa-b8ee-257f1cecadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAREVAL big\n",
    "rank_range = np.arange(-20, 21)\n",
    "#define a new colormap (colors from -20-0-20 = 41 colors)\n",
    "redblue = mpl.colormaps['RdYlBu_r'].resampled(len(rank_range))\n",
    "newcolors = redblue(np.linspace(0, 1, len(rank_range)))\n",
    "\n",
    "color_values = [0, 0.09, 0.18, 0.27, 0.36, \n",
    "                0.45, \n",
    "                0.54, 0.65, 0.75, 0.89, 0.99]\n",
    "\n",
    "colors = [redblue(i) for i in color_values]\n",
    "\n",
    "newcolors[:10, :] = colors[0]\n",
    "newcolors[10:14, :] = colors[1]\n",
    "newcolors[14:16, :] = colors[2]\n",
    "newcolors[17:19, :] = colors[3]\n",
    "newcolors[19, :] = colors[4]\n",
    "newcolors[20, :] = colors[5]\n",
    "newcolors[21, :] = colors[6]\n",
    "newcolors[22:24, :] = colors[7]\n",
    "newcolors[24:27, :] = colors[8]\n",
    "newcolors[27:31, :] = colors[9]\n",
    "newcolors[31:, :] = colors[10]\n",
    "\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "#select and reorder dataframe\n",
    "df = all_df\n",
    "atlas = 'STG' #'wholebrain'    \n",
    "training_data= 'individual' #'group'\n",
    "task_training_size = 'small_train' #'medium_train' #'big_train' \n",
    "comparaison = 'duo' if not solo_soundnet else 'solo'\n",
    "if big and small:\n",
    "    models_size = 'all'\n",
    "elif big:\n",
    "    models_size = 'big'\n",
    "else:\n",
    "    models_size = 'small'\n",
    "\n",
    "\n",
    "df_selected = df.loc[(df['finetune'] == 'conv4') & (df['atlas'] == atlas) & (df['training_data'] == training_data)]\n",
    "if task_training_size == 'small_train':\n",
    "    tasks_list=list(task_training_small['task'])\n",
    "elif task_training_size == 'medium_train':\n",
    "    tasks_list=list(task_training_medium['task'])\n",
    "else:\n",
    "    tasks_list=list(task_training_big['task'])\n",
    "    \n",
    "df_selected = df_selected.loc[df_selected['test'].isin(tasks_list)]\n",
    "test_order = pd.CategoricalDtype(tasks_list, ordered=True)\n",
    "df_selected['test']=df_selected['test'].astype(test_order)\n",
    "df_selected.sort_values(['test', 'subject'], inplace=True)\n",
    "\n",
    "#automatic sizes range for graph\n",
    "unique_rank_values = np.sort(df_selected['diff_rank'].unique())\n",
    "sizes=[100, 150]+[200]*2+[250]*3+[300]*4+[350]*10\n",
    "sizes_range=[]\n",
    "for rank in unique_rank_values:\n",
    "    if not np.isnan(rank):\n",
    "        i = int(abs(rank))\n",
    "        sizes_range.append(sizes[i])\n",
    "\n",
    "#legend\n",
    "labels = ['-11 to -20', '-7 to -10', '-4 to -6', '-2 to -3', '-1', '0', '+1', '+2 to +3', '+4 to +6', '+7 to +10', '+11 to +20'] #HEAREVAL full\n",
    "markersizes = [18, 16.8, 15.5, 14.2, 12.5, 10, 12.5, 14.2, 15.5, 16.8, 18]\n",
    "legend_circles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], markersize=markersizes[i], label=labels[i]) \n",
    "                  for i in range(len(markersizes))]\n",
    "\n",
    "#graph \n",
    "graph_height = 455/19*len(tasks_list)\n",
    "px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "plt.subplots(figsize=(841*px, graph_height*px))\n",
    "\n",
    "ax = sns.scatterplot(data=df_selected, x=\"subject\", y=\"test\", \n",
    "                hue=\"diff_rank\", size=\"diff_rank\",sizes=sizes_range, palette = newcmp, hue_norm = (-20, 20),\n",
    "               legend=True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1), reverse=True, handles=legend_circles) \n",
    "ax.margins(y=0.2)\n",
    "ax.set_title(atlas)\n",
    "plt.savefig('/home/maellef/Results/finefriends/figures/figure_papier/HEAR_EVAL_{}_{}_{}_{}_{}_models_sept2024.png'.format(training_data, atlas, \n",
    "                                                                                                                task_training_size,\n",
    "                                                                                                             comparaison, models_size), \n",
    "            bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f25cc4-08b1-4bbc-88ff-f0390f37baa1",
   "metadata": {},
   "source": [
    "rank figures - different scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8aa116-96cb-42b5-bb74-619c118b2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAREVAL small\n",
    "rank_range = np.arange(-10, 11)\n",
    "#define a new colormap (colors from -20-0-20 = 41 colors)\n",
    "redblue = mpl.colormaps['RdYlBu_r'].resampled(len(rank_range))\n",
    "newcolors = redblue(np.linspace(0, 1, len(rank_range)))\n",
    "\n",
    "color_values = [0, 0.11, 0.22, 0.33, 0.45, 0.58, 0.72, 0.85, 0.99]\n",
    "colors = [redblue(i) for i in color_values]\n",
    "\n",
    "newcolors[:5, :] = colors[0]\n",
    "newcolors[5:7, :] = colors[1]\n",
    "newcolors[7:9, :] = colors[2]\n",
    "newcolors[9, :] = colors[3]\n",
    "newcolors[10, :] = colors[4]\n",
    "newcolors[11, :] = colors[5]\n",
    "newcolors[12:14, :] = colors[6]\n",
    "newcolors[14:16, :] = colors[7]\n",
    "newcolors[16:, :] = colors[8]\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "#select and reorder dataframe\n",
    "df = all_df\n",
    "atlas = 'wholebrain' #'STG'     \n",
    "df_selected = df.loc[(df['finetune'] == 'conv4') & (df['atlas'] == atlas)]\n",
    "\n",
    "tasks_list=list(task_training_small['task']) #task_training_small task_training_medium task_training_big\n",
    "training_Size = 'small_tr'\n",
    "df_selected = df_selected.loc[df_selected['test'].isin(tasks_list)]\n",
    "test_order = pd.CategoricalDtype(tasks_list, ordered=True)\n",
    "df_selected['test']=df_selected['test'].astype(test_order)\n",
    "df_selected.sort_values(['test', 'subject'], inplace=True)\n",
    "\n",
    "#automatic sizes range for graph\n",
    "unique_rank_values = np.sort(df_selected['diff_rank'].unique())\n",
    "sizes=[100, 150]+[200]*2+[275]*2+[350]*5\n",
    "sizes_range=[]\n",
    "for rank in unique_rank_values:\n",
    "    if not np.isnan(rank):\n",
    "        i = int(abs(rank))\n",
    "        sizes_range.append(sizes[i])\n",
    "\n",
    "#legend\n",
    "labels = ['-6 to -10', '-4 to -5', '-2 to -3', '-1', '0', '+1', '+2 to +3', '+4 to +5', '+6 to +10']#HEAREVAL small\n",
    "markersizes = [18, 16.3, 14.2, 12.5, 10, 12.5, 14.2, 16.3, 18]\n",
    "legend_circles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], markersize=markersizes[i], label=labels[i]) \n",
    "                  for i in range(len(markersizes))]\n",
    "\n",
    "#graph \n",
    "graph_height = 455/19*len(tasks_list)\n",
    "px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "plt.subplots(figsize=(841*px, graph_height*px))\n",
    "\n",
    "ax = sns.scatterplot(data=df_selected, x=\"subject\", y=\"test\", \n",
    "                hue=\"diff_rank\", size=\"diff_rank\",sizes=sizes_range, palette = newcmp, hue_norm = (-10, 10),\n",
    "               legend=True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1), reverse=True, handles=legend_circles) \n",
    "ax.set_title(atlas)\n",
    "ax.margins(y=0.2)\n",
    "plt.savefig('/home/maellef/Results/figures/figure_papier/HEAR_EVAL_{}_{}_duo_small_model_diff_2024.png'.format(atlas, training_Size), \n",
    "            bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea8559-fe57-4cfd-a6b4-d88f11d18683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAREVAL big\n",
    "rank_range = np.arange(-20, 21)\n",
    "#define a new colormap (colors from -20-0-20 = 41 colors)\n",
    "redblue = mpl.colormaps['RdYlBu_r'].resampled(len(rank_range))\n",
    "newcolors = redblue(np.linspace(0, 1, len(rank_range)))\n",
    "\n",
    "color_values = [0, 0.11, 0.22, 0.33, 0.45, 0.58, 0.72, 0.85, 0.99]\n",
    "colors = [redblue(i) for i in color_values]\n",
    "\n",
    "newcolors[:10, :] = colors[0]\n",
    "newcolors[10:15, :] = colors[1]\n",
    "newcolors[15:19, :] = colors[2]\n",
    "newcolors[19, :] = colors[3]\n",
    "newcolors[20, :] = colors[4]\n",
    "newcolors[21, :] = colors[5]\n",
    "newcolors[22:26, :] = colors[6]\n",
    "newcolors[26:31, :] = colors[7]\n",
    "newcolors[31:, :] = colors[8]\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "#select and reorder dataframe\n",
    "df = all_df\n",
    "atlas = 'STG' #'wholebrain'    \n",
    "df_selected = df.loc[(df['finetune'] == 'conv4') & (df['atlas'] == atlas)]\n",
    "\n",
    "tasks_list=list(task_training_small['task']) #task_training_small task_training_medium task_training_big\n",
    "training_Size = 'small_tr'\n",
    "df_selected = df_selected.loc[df_selected['test'].isin(tasks_list)]\n",
    "test_order = pd.CategoricalDtype(tasks_list, ordered=True)\n",
    "df_selected['test']=df_selected['test'].astype(test_order)\n",
    "df_selected.sort_values(['test', 'subject'], inplace=True)\n",
    "\n",
    "#automatic sizes range for graph\n",
    "unique_rank_values = np.sort(df_selected['diff_rank'].unique())\n",
    "sizes=[100, 150]+[200]*4+[275]*5+[350]*10\n",
    "sizes_range=[]\n",
    "for rank in unique_rank_values:\n",
    "    if not np.isnan(rank):\n",
    "        i = int(abs(rank))\n",
    "        sizes_range.append(sizes[i])\n",
    "\n",
    "#legend\n",
    "labels = ['-11 to -20', '-6 to -10', '-2 to -5', '-1', '0', '+1', '+2 to +5', '+6 to +10', '+11 to +20'] #HEAREVAL full\n",
    "markersizes = [18, 16.3, 14.2, 12.5, 10, 12.5, 14.2, 16.3, 18]\n",
    "legend_circles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], markersize=markersizes[i], label=labels[i]) \n",
    "                  for i in range(len(markersizes))]\n",
    "\n",
    "#graph \n",
    "graph_height = 455/19*len(tasks_list)\n",
    "px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "plt.subplots(figsize=(841*px, graph_height*px))\n",
    "\n",
    "ax = sns.scatterplot(data=df_selected, x=\"subject\", y=\"test\", \n",
    "                hue=\"diff_rank\", size=\"diff_rank\",sizes=sizes_range, palette = newcmp, hue_norm = (-20, 20),\n",
    "               legend=True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1), reverse=True, handles=legend_circles) \n",
    "ax.margins(y=0.2)\n",
    "ax.set_title(atlas)\n",
    "plt.savefig('/home/maellef/Results/figures/figure_papier/HEAR_EVAL_{}_{}_duo_small_model_diff2_2024.png'.format(atlas, training_Size), \n",
    "            bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2c378-f203-40a2-ab40-604da9fdca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEAREVAL small rank\n",
    "rank_range = np.arange(10, 0, -1)\n",
    "#define a new colormap (colors from -20-0-20 = 41 colors)\n",
    "redblue = mpl.colormaps['RdYlBu_r'].resampled(len(rank_range))\n",
    "newcolors = redblue(np.linspace(0, 1, len(rank_range)))\n",
    "\n",
    "color_values = [0, 0.11, 0.22, 0.33, 0.45, 0.58, 0.64, 0.72, 0.85, 0.99]\n",
    "colors = [redblue(i) for i in color_values]\n",
    "\n",
    "for i in range(10):\n",
    "    y = 10-(i+1)\n",
    "    newcolors[i, :] = colors[y]\n",
    "\n",
    "newcmp = ListedColormap(newcolors)\n",
    "\n",
    "#select and reorder dataframe\n",
    "df = all_df\n",
    "atlas = 'wholebrain'#'STG'  \n",
    "df_selected = df.loc[(df['finetune'] == 'conv4') & (df['atlas'] == atlas)]\n",
    "df_selected = pd.concat([df_selected, soundnet_median_ranks], axis=0, ignore_index=True)\n",
    "test_order = pd.CategoricalDtype(['Gunshot','Beijing Opera','NSynth Pitch 5h','NSynth Pitch 50h','ESC-50',\n",
    "              'Speech commands 5h','CREMA-D','FSD50K','GTZAN Genre','GTZAN Music/Speech',\n",
    "              'Speech commands full','Mridangam Stroke','Libricount','Maestro 5h',\n",
    "              'Beehive','VoxLingua107 top 10','DCASE 2016','Vocal Imitation','Mridangam Tonic'], ordered=True)\n",
    "\n",
    "df_selected['test']=df_selected['test'].astype(test_order)\n",
    "df_selected.sort_values(['test', 'subject'], inplace=True)\n",
    "\n",
    "#automatic sizes range for graph\n",
    "unique_rank_values = np.sort(df_selected['rank'].unique())\n",
    "sizes=[350, 275, 200, 150, 100, 100, 150, 200, 275, 350]\n",
    "sizes_range=[]\n",
    "for rank in unique_rank_values:\n",
    "    if not np.isnan(rank):\n",
    "        i = int(abs(rank))\n",
    "        sizes_range.append(sizes[2])\n",
    "\n",
    "#legend\n",
    "labels = ['10', '9', '8', '7', '6', '5', '4', '3', '2', '1']\n",
    "markersizes = [18, 16.3, 14.2, 12.5, 10, 12.5, 14.2, 16.3, 18]\n",
    "legend_circles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[i], markersize=markersizes[i], label=labels[i]) \n",
    "                  for i in range(len(markersizes))]\n",
    "\n",
    "#graph\n",
    "ax = sns.scatterplot(data=df_selected, x=\"subject\", y=\"test\", \n",
    "                hue=\"rank\", size=\"rank\",sizes=sizes_range, palette = newcmp, hue_norm = (1, 10),\n",
    "               legend=True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1), reverse=True, handles=legend_circles) \n",
    "ax.set_title(atlas)\n",
    "plt.savefig('/home/maellef/Results/figures/figure_papier/HEAR_EVAL_{}_small_rank_2024.png'.format(atlas), \n",
    "            bbox_inches='tight', transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
